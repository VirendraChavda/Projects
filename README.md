# ğŸš€ Deployment Workflow Guide  
### For Multi-App Projects with Docker Desktop + Kubernetes (Kustomize)

This document explains **what to do after finishing local development** of any app inside the `Applications/` folder â€” including how to build the Docker image, apply Kubernetes YAMLs, and promote across environments (`dev`, `test`, `prod`).

---

## ğŸ“ Folder Structure Overview

```
D:\Projects
â”‚
â”œâ”€â”€ Applications/
â”‚   â”œâ”€â”€ llm-app-frontend/
â”‚   â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â”œâ”€â”€ pyproject.toml
â”‚   â”‚   â”œâ”€â”€ poetry.lock
â”‚   â”‚   â””â”€â”€ tests/
â”‚   â”‚
â”‚   â””â”€â”€ llm-app-backend/
â”‚       â”œâ”€â”€ app/
â”‚       â”œâ”€â”€ Dockerfile
â”‚       â”œâ”€â”€ pyproject.toml
â”‚       â”œâ”€â”€ poetry.lock
â”‚       â””â”€â”€ tests/
â”‚
â””â”€â”€ Cluster/
    â”œâ”€â”€ llm-app-frontend/
    â”‚   â”œâ”€â”€ base/
    â”‚   â”‚   â”œâ”€â”€ deployment.yaml
    â”‚   â”‚   â”œâ”€â”€ service.yaml
    â”‚   â”‚   â””â”€â”€ kustomization.yaml
    â”‚   â””â”€â”€ overlays/
    â”‚       â”œâ”€â”€ dev/kustomization.yaml
    â”‚       â”œâ”€â”€ test/kustomization.yaml
    â”‚       â””â”€â”€ prod/kustomization.yaml
    â”‚
    â””â”€â”€ llm-app-backend/
        â”œâ”€â”€ base/
        â”‚   â”œâ”€â”€ deployment.yaml
        â”‚   â”œâ”€â”€ service.yaml
        â”‚   â””â”€â”€ kustomization.yaml
        â””â”€â”€ overlays/
            â”œâ”€â”€ dev/kustomization.yaml
            â”œâ”€â”€ test/kustomization.yaml
            â””â”€â”€ prod/kustomization.yaml
```

---

## ğŸ§± Step 1 â€” Verify App Works Locally

Each application (frontend or backend) is an independent Poetry project.

```bash
cd Applications\llm-app-backend
poetry install
poetry run pytest      # optional tests
poetry run python -m app
```

âœ… If it runs correctly â€” proceed to containerization.

---

## ğŸ§° Step 2 â€” Build the Docker Image

Each project folder already contains a Dockerfile.  
Run the following inside your application folder:

```bash
docker build -t llm-app-backend:dev .
```

For frontend:
```bash
docker build -t llm-app-frontend:dev .
```

### Optional GPU check (PyTorch or CUDA)
```bash
docker run --rm -it --gpus all llm-app-backend:dev   python -c "import torch;print(torch.cuda.is_available(), torch.cuda.get_device_name(0))"
```

If you see your GPU name â€” CUDA works.

---

## â˜¸ï¸ Step 3 â€” Confirm Kubernetes Is Running

Make sure Docker Desktopâ€™s built-in Kubernetes cluster is active.

```bash
kubectl get nodes
```

You should see:
```
NAME             STATUS   ROLES           AGE   VERSION
docker-desktop   Ready    control-plane   2d    v1.30.x
```

If not, open Docker Desktop â†’ Settings â†’ Kubernetes â†’ **Enable Kubernetes** â†’ wait until â€œKubernetes is runningâ€.

---

## ğŸ§© Step 4 â€” Deploy to Kubernetes (Dev Environment)

Kubernetes manifests live under the `Cluster/` folder.  
Each app has its own **base** (common YAML) and **overlays** (per-environment configuration).

To deploy the **backend**:
```bash
kubectl apply -k Cluster/llm-app-backend/overlays/dev
```

To deploy the **frontend**:
```bash
kubectl apply -k Cluster/llm-app-frontend/overlays/dev
```

You can repeat the same command for `test` or `prod` overlays later.

---

## ğŸ” Step 5 â€” Verify Deployment

Check namespaces and pods:

```bash
kubectl get ns
kubectl get pods -n dev
```

Expected output:
```
NAME                                 READY   STATUS    RESTARTS   AGE
llm-app-backend-5f9f56b4b9-b9dps     1/1     Running   0          30s
llm-app-frontend-6c9f4577d9-4f5lx    1/1     Running   0          28s
```

View logs:
```bash
kubectl logs -n dev deploy/llm-app-backend
```

---

## âš™ï¸ Step 6 â€” Update Image After Code Changes

When you modify code:

1. Rebuild your image:
   ```bash
   docker build -t llm-app-backend:dev .
   ```

2. Reapply the overlay:
   ```bash
   kubectl apply -k Cluster/llm-app-backend/overlays/dev
   ```

Kubernetes will detect the new image and roll out a fresh pod automatically.

---

## ğŸš€ Step 7 â€” Promote Between Environments

When your app works well in **dev**, promote it to higher environments.

| Environment | Command | Description |
|--------------|----------|-------------|
| Dev | `kubectl apply -k overlays/dev` | Default testing setup |
| Test | `kubectl apply -k overlays/test` | Uses different namespace / tag |
| Prod | `kubectl apply -k overlays/prod` | Production deployment |

Each overlay file sets:
- the correct **namespace** (`dev`, `test`, `prod`)
- the **Docker image tag** (`:dev`, `:test`, `:prod`)
- optional replica count or limits

---

## ğŸ§  Step 8 â€” Check GPU Inside Pod (Optional)

```bash
kubectl -n dev exec -it deploy/llm-app-backend --   python -c "import torch;print(torch.cuda.is_available(), torch.cuda.get_device_name(0))"
```

If it returns your GPU name â†’ GPU plugin and image are configured correctly.

---

## ğŸ§¹ Step 9 â€” Cleanup or Redeploy

Remove a deployment:
```bash
kubectl delete -k Cluster/llm-app-backend/overlays/dev
```

Remove the namespace (optional):
```bash
kubectl delete namespace dev
```

Recreate later with:
```bash
kubectl create namespace dev
```

---

## ğŸ§© Step 10 â€” Troubleshooting

| Problem | Cause | Fix |
|----------|--------|-----|
| `ImagePullBackOff` | Image tag mismatch or not built | Ensure `docker build -t <name>:<tag> .` matches overlay image tag |
| `namespace not found` | Namespace deleted or missing | Recreate with `kubectl create namespace dev` |
| Pod stuck in `Pending` | GPU resource requested but plugin missing | Either install NVIDIA device plugin or remove GPU limit |
| `Kubernetes not running` | Docker Desktop setting off | Enable it under Docker Desktop â†’ Settings â†’ Kubernetes |

---

## ğŸ“¦ Common Commands Reference

```bash
# List all namespaces
kubectl get ns

# List pods in a namespace
kubectl get pods -n dev

# Check logs of a pod
kubectl logs -n dev deploy/llm-app-backend

# Describe a pod in detail
kubectl describe pod <pod-name> -n dev

# Reapply configuration
kubectl apply -k Cluster/llm-app-backend/overlays/dev

# Delete deployment
kubectl delete -k Cluster/llm-app-backend/overlays/dev
```

---

## ğŸ§­ Summary Workflow

| Step | Action | Tool |
|------|---------|------|
| 1 | Develop & test locally with Poetry | Python / Poetry |
| 2 | Build Docker image | Docker |
| 3 | Ensure Kubernetes is running | Docker Desktop |
| 4 | Apply Kustomize overlay (dev/test/prod) | kubectl |
| 5 | Check pods & logs | kubectl |
| 6 | Promote image tags to higher environments | kubectl / CI/CD |

---

## ğŸ§± How Base & Overlays Work (Quick Reminder)

- **`base/`** â†’ Common YAML files (Deployment + Service) shared by all environments  
- **`overlays/`** â†’ Environment-specific tweaks  
  - `namespace` (`dev`, `test`, `prod`)  
  - `image tag` (e.g., `myapp:dev`, `myapp:test`)  
  - optional replica counts or resource limits  

You deploy using overlays, e.g.:

```bash
kubectl apply -k Cluster/llm-app-backend/overlays/dev
```

Kustomize automatically merges your base + overlay YAMLs and applies the final configuration to the cluster.

---

## âœ… Final Notes

- All applications (`frontend`, `backend`, etc.) can share the **same Kubernetes cluster** and namespaces.
- Namespaces are persistent â€” they survive PC and Docker restarts.
- You only rebuild images when app code changes.
- You only modify YAMLs when deployment structure or resources change.

---

**Authorâ€™s Note:**  
This README covers a self-contained, multi-application workflow for local or single-node clusters using **Docker Desktop + Kubernetes + Kustomize**.  
It ensures reproducible, GPU-aware deployments without needing Helm or cloud infrastructure.

---